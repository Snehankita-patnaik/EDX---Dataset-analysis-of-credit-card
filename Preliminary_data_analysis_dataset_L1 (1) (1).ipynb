{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdaqf6vgTTg4"
   },
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsPreliminary_data_analysis_dataset_L126905723-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2VgPk2mTThF"
   },
   "source": [
    "# **Secure analysis of a credit card dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkRBtAnQTThK"
   },
   "source": [
    "The purpose of this lab is to make a preliminary data analysis in a security area based on a credit card clients dataset with helping frameworks & libraries.\n",
    "\n",
    "After completing this lab, you will be able to\n",
    "\n",
    "1.  Explore the credit card clients dataset and calculate the main statistical indicators.\n",
    "2.  Build different dependencies among the existing attributes of the dataset.\n",
    "3.  Visualize the data analysis results with various plot types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHwOXFUpTThK"
   },
   "source": [
    "## Outline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHSc4iksTThP"
   },
   "source": [
    "*   Materials and Methods\n",
    "*   General part\n",
    "    *   Libraries Import\n",
    "    *   Dataset Exploration\n",
    "    *   Dataset Visualization\n",
    "*   Tasks\n",
    "*   Author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4t5rpwITThQ"
   },
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoNAvKmglBm-"
   },
   "source": [
    "## Materials and Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOgyr5RSlI40"
   },
   "source": [
    "The data that we are going to use for this is a subset of an open source default of a credit card clients dataset from the UCI ML repository: [https://archive.ics.uci.edu/ml/citation_policy.html](https://archive.ics.uci.edu/ml/citation_policy.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsPreliminary_data_analysis_dataset_L126905723-2022-01-01).\n",
    "\n",
    "> This dataset is public available for research. The details are described in \\[Yeh et al., 2009].\n",
    "\n",
    "Please include this citation if you plan to use this database:\n",
    "Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.\n",
    "\n",
    "During the work, the task of preliminary data analysis of clients' default payments is solved. In essence, the task is a matter of a security management area, where the results of preliminary data analysis of the estimated probability of default payments should be more helpful to prevent different dangerous states.\n",
    "\n",
    "In this lab, we will try to give answers to a set of questions that may be relevant when analyzing credit card clients' data:\n",
    "\n",
    "1.  What is the average age of all the clients?\n",
    "2.  What part of clients out of the whole set has an issue with the default payment in the next month?\n",
    "3.  How many clients are married and single?\n",
    "4.  What are percent proportions for non-unique values of the variables?\n",
    "5.  Which are the values with very high dependences among all the features based on the full correlation matrix?\n",
    "6.  How can we identify new trends in a client's operations roadmap?\n",
    "7.  How can we define the boundaries (where most data points are tightly concentrated) for the feature pairs?\n",
    "8.  What is the share of clients in our DataFrame who are ready for their default payment next month?\n",
    "9.  What are the mean values ​​of numerical features among the attracted clients (the average age of the clients and the average delay of their repayment status)?\n",
    "10. What was the average repayment status in September 2005 for a typical client who was going to pay in the next month?\n",
    "11. What part of clients paid duly in September 2005?\n",
    "\n",
    "In addition, we will make a visualization of our dataset to plan security management actions more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6D_qFGMqUaN"
   },
   "source": [
    "[Pandas](http://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsPreliminary_data_analysis_dataset_L126905723-2022-01-01) is a Python library that provides extensive means for data analysis. Data scientists often work with data stored in table formats like .csv, .tsv, or .xlsx. Pandas makes it very convenient to load, process, and analyze such tabular data using SQL-like queries. In conjunction with Matplotlib and Seaborn, Pandas provides a wide range of opportunities for visual analysis of tabular data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS8UOiQOTThR"
   },
   "source": [
    "## Libraries Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6YFQWnQHKui"
   },
   "source": [
    "Download the data using a URL and rename it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bN5NPujBTThV"
   },
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\n",
    "!mv -f 'default of credit card clients.xls' 'CreditCard.xls'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BnB2tiSZh4R"
   },
   "source": [
    "Alternative URL for the dataset downloading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORfD1EUiZuLK"
   },
   "outputs": [],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Preliminary_data_analysis_dataset_L1/default_of_credit_card_clients.xls\n",
    "!mv -f 'default of credit card clients.xls' 'CreditCard.xls'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGH5DhdLHmOp"
   },
   "source": [
    "Import the libraries necessary for this lab. We can add some aliases (such as pd, plt, np, sns) to make the libraries easier to use in our code and set a default figure size for further plots. Ignore the warnings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdVcijxmTTha"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evtbog-C13Z2"
   },
   "source": [
    "Further specify the value of the `precision` parameter equal to 3 to display three decimal signs (instead of 6 as default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwMoaJLV13Z3"
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"precision\", 3)\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmrZguMTThd"
   },
   "source": [
    "## Dataset Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvMGO3m4TThd"
   },
   "source": [
    "In this section you will explore the sourse dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ-GKs1J13Zz"
   },
   "source": [
    "Let's read the data and look at the first 7 rows using the `head` method. The number of the output rows from the dataset is determined by the `head` method parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wamAf3HLm_FS"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('CreditCard.xls', header = 1, index_col = 0)\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjzxXtUp13Z3"
   },
   "source": [
    "### Let's look at the dataset size, feature names and their types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53xc7fWwoOMD"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtO5KNTV13Z5"
   },
   "source": [
    "The dataset contains `30 000` objects (rows), for each of which `24` features are set (columns), including 1 target feature (`default payment next month`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVOmrfNH3pRs"
   },
   "source": [
    "### Attribute Information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSTqzHlbKWoa"
   },
   "source": [
    "Output the column (feature) names:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uuAnHe3nvCh"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SDpd1dxKeq4"
   },
   "source": [
    "Input features (column names):\n",
    "\n",
    "1.  `ID` - Client identification number (numeric).\n",
    "2.  `LIMIT_BAL` - Amount of the given credit (NT dollar): it includes both an individual consumer's credit and their family (supplementary) credit (numeric).\n",
    "3.  `SEX` - Gender (categorical: `1` = male; `2` = female).\n",
    "4.  `EDUCATION` - Education (categorical: `1` = graduate school; `2` = university; `3` = high school; `4` = others).\n",
    "5.  `MARRIGE` - Marital status (categorical: `1` = married; `2` = single; `3` = others).\n",
    "6.  `AGE` - Age (year, numeric).\n",
    "7.  `PAY_0` – `PAY_6` - History of past payment. We tracked the previous monthly payment records (from April to September, 2005) as follows: `PAY_0` = the repayment status in September, 2005; `PAY_1` = the repayment status in August, 2005; . . .; `PAY_6` = the repayment status in April, 2005. The measurement scale for the repayment status is: `-1` = paid duly; `1` = payment delay for one month; `2` = payment delay for two months; . . .; `8` = payment delay for eight months; `9` = payment delay for nine months and above.\n",
    "8.  `BILL_AMT1` – `BILL_AMT6` - Amount of bill statement (NT dollar). `BILL_AMT1` = amount of bill statement in September, 2005; `BILL_AMT2` = amount of bill statement in August, 2005; . . .; `BILL_AMT6` = amount of bill statement in April, 2005.\n",
    "9.  `PAY_AMT1` – `PAY_AMT6` - The amount of previous payment (NT dollar). `PAY_AMT1` = the amount paid in September, 2005; `PAY_AMT2` = the amount paid in August, 2005; . . .; `PAY_AMT6` = the amount paid in April, 2005.\n",
    "\n",
    "Output feature (desired target):\n",
    "\n",
    "10. `default payment next month` - a binary variable, default payment (binary: Yes = `1`, No = `0`), as the response variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6SaYJgj13Z6"
   },
   "source": [
    "To see general information on all the dataframe features (columns), we use the **`info`** method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCV1-dAJ13Z6"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic2B0eP0PshN"
   },
   "source": [
    "As you can see, the dataset is full, no pass (`non-null`), so there is no need to fill the gaps. The dataset contains 24 integer (`int64`) features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2I_fcSH13Z8"
   },
   "source": [
    "Method **`describe`** shows the main statistical characteristics of the dataset for each numerical feature (`int64` type): existing values number, mean, standard deviation, range, min & max, 0.25, 0.5 and 0.75 quartiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGuHLQMl13Z8"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vz-xyMpdVC3I"
   },
   "source": [
    "The `Mean` row shows the feature average, `STD` is a RMS (Root Mean Square) deviation, `min`, `max` - minimum and maximum values, `25%`, `50%`, ` 75%  `- quarters that split the dataset (or part of it) into four groups containing approximately an equal number of observations (rows). For example, the age of about a quarter of all the clients  (`AGE`) is about 28.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUuf9K8JQ6-c"
   },
   "source": [
    "In general, according to the data, it is impossible to say that there are outliers in the data. However, such an inspection is not enough, it is desirable to still see the charts of the target feature dependence from each input feature. We will do it later when we visualize features and dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0-sAGXl13Z_"
   },
   "source": [
    "For boolean features (type `bool` or via numerical values: `1` or `0`) you can use the **`value_counts`** method. Let's look at the target feature (`default payment next month`) distribution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjchGbdx13aA"
   },
   "outputs": [],
   "source": [
    "df[\"default payment next month\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8P92yMFY13aB"
   },
   "source": [
    "77.9% of the clients out of 30 000 have an issue when the value of the variable `default payment next month` equals `0`.\n",
    "\n",
    "Let's look at the client distribution by the variable `MARRIAGE`. Specify the value of the `normalize = True` parameter to view relative frequencies, but not absolute.\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with your Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPqBQPAj13aC"
   },
   "outputs": [],
   "source": [
    "df[\"MARRIAGE\"].##YOUR CODE GOES HERE##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z57YgF-mgjNc"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "df[\"MARRIAGE\"].value_counts(normalize = True)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjpEnpfUTaKa"
   },
   "source": [
    "As we can see, 45,5% (`0.455`) of the clients are married and 53,2% (`0.532`) are single, which must be taken into account when planning security campaigns to manage clients' operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuChKGTT13aC"
   },
   "source": [
    "### Counting the unique values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBjWyQ1jLLCN"
   },
   "source": [
    "One of the stages of preliminary data analysis is counting unique values in the dataset which is presented now as a `DataFrame`. Here and further we will work with the `DataFrame`.\n",
    "\n",
    "For this, firstly, we will count non-unique values, then we will define the columns for dropping based on values counting. Then, we will compare dimensionalities of our dataset after the dropping process and show a table with these indicators and their percent proportions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cmkch-0TGQhM"
   },
   "outputs": [],
   "source": [
    "from numpy import unique\n",
    "\n",
    "counts = df.nunique() \n",
    "for_delete = [i for i,v in enumerate(counts) if v == 1]\n",
    "if for_delete == []:\n",
    "  print('Numbers of the useless columns are: empty (it means all our columns are useful).')\n",
    "else:\n",
    "  print('Numbers of the useless columns are:', for_delete)\n",
    "df.drop(for_delete, axis = 1, inplace = True)\n",
    "print('\\nDimensionalities of our dataset after dropping are:', '\\n', 'objects (clients) = ', '\\t', '\\t', df.shape[0],\n",
    "      '\\n', 'features (column names) = ', '\\t', df.shape[1])\n",
    "if (df.shape[0] == 30000) and (df.shape[1] == 24):\n",
    "  print('\\nThus, dimensionalities of our dataset after the dropping process have not changed.\\n')\n",
    "print(\"Numbers (Nr) of unique values (UV) for each column and its percent (%):\\n\")\n",
    "print('Nr \\t UV \\t %')\n",
    "print('======================')\n",
    "for i in range(df.shape[1]):\n",
    "    num = len(unique(df.iloc[:, i]))\n",
    "    percentage = float(num) / df.shape[0] * 100\n",
    "    print('%d\\t %d\\t %.1f' % (i, num, percentage))\n",
    "print('======================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fHxcN6uOE0K"
   },
   "source": [
    "One way to quantify the relationship between variables pairs in the `DataFrame` is to use the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsPreliminary_data_analysis_dataset_L126905723-2022-01-01) which is a measure of the linear association between variables pairs (each with each). It takes on a value between -1 and 1, where:\n",
    "\n",
    "**-1 indicates a perfect negative linear correlation**\n",
    "\n",
    "**0 indicates no linear correlation**\n",
    "\n",
    "**1 indicates a perfect positive linear correlation**\n",
    "\n",
    "The higher the correlation coefficient is from zero, the stronger the relationship between the two variables is. But, in some cases, we would want to understand the correlation between more than just one pair of variables. In these cases, we can create a `correlation matrix` which is a square table that shows the correlation coefficients between all pairwise combinations of variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtaCg8TmKqGf"
   },
   "outputs": [],
   "source": [
    "df.corr().style.background_gradient(cmap = 'BrBG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkJyuPIW2tjE"
   },
   "source": [
    "We can see (with positive values) very high dependences among `PAY_x` features based on the full correlation matrix shown above. For instance, the coefficient correlation is equal to `0.817` between `PAY_5` and `PAY_6`. These positive high correlations between the pairs of variables confirm that these variables are strong and likely important in our dataset. Also, for a positive increase in one variable, there is also a positive increase in the second variable. If the correlation between two variables is about 0 (for instance, the coefficient correlation is equal to `0.082` between `PAY_6` and `EDUCATION`), there is no linear relationship between them.\n",
    "\n",
    "Working with quite big tables or matrices (DataFrames) isn't easy, so sometimes we have to decrease the size of a table by using necessary columns only. You need to indicate the names of the necessary columns or convert these names to a separate list (by `tolist()` method) as well as indicate a range of column numbers. For instance, if we only need the first five columns (`[0:6]`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RUxRZX03z7D"
   },
   "outputs": [],
   "source": [
    "df_example = pd.DataFrame(df, columns=df.columns.tolist()[0:6])\n",
    "df_example.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WpCvDVQ6mpg"
   },
   "source": [
    "Let's decrease our dataset based on unique values defined earlier and use only values which are higher or equal 20%.\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with your Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47eryWqH4CS6"
   },
   "outputs": [],
   "source": [
    "df_unique = ##YOUR CODE GOES HERE##\n",
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnkhnOdr60Th"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "df_unique = pd.DataFrame(df, columns = df.columns.tolist()[11:23])\n",
    "df_unique.head()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nntiTuMOqCG"
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(17, 12))\n",
    "plt.matshow(df_unique.corr(), fignum=f.number)\n",
    "plt.xticks(range(df_unique.shape[1]), df_unique.columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(df_unique.shape[1]), df_unique.columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix\\n\\n', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FBBnPZ1Uiy5"
   },
   "source": [
    "As you can see, the `very light area` (with positive values) confirms quite high dependences among `BILL_AMTx` features (for instance, coefficient correlation is equal to `0.946` between `BILL_AMT6` and `BILL_AMT5`). In addition, security specialists can use changes in correlation statistics to identify new trends in a client's operations roadmap.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHsAC2Vr13aO"
   },
   "source": [
    "### Studying separate dependencies of variables pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhFtQbGX13aP"
   },
   "source": [
    "We can build subsets of variables pairs, which can show us their distribution and direct dependencies. For this, as it was shown before, you should perform the same operations by analogy with `tolist()` method, but only for two variables (features). For instance, let's create a new DataFrame which consists of two variables: `AGE` and `LIMIT_BAL`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9tABUDp13aQ"
   },
   "outputs": [],
   "source": [
    "df_ALB = pd.DataFrame(df, columns=['LIMIT_BAL','AGE'])\n",
    "sns.jointplot(x='AGE', y='LIMIT_BAL', data = df_ALB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BqG93kcXW57"
   },
   "source": [
    "As we can see, most of the data points are tightly concentrated in the area with boundaries (on average) between `25` and `60` years, and in a range of amount of the given credit before `0.5 NT dollars`. This conclusion can be used as a factor of close attention to this risk group from a security specialist's side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze8tUYHP13aT"
   },
   "source": [
    "Let's try to define the boundaries (where the most data points are tightly concentrated) for the next two features: `PAY_AMT5` and `LIMIT_BAL` and the appropriate DataFrame `df_LBP`.\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with your Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DchqdQ0_13aU"
   },
   "outputs": [],
   "source": [
    "df_LBP = pd.DataFrame##YOUR CODE GOES HERE##\n",
    "sns.jointplot##YOUR CODE GOES HERE##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFDBIaFrg--8"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "df_LBP = pd.DataFrame(df, columns=['LIMIT_BAL','PAY_AMT5'])\n",
    "sns.jointplot(x='PAY_AMT5', y='LIMIT_BAL', data = df_LBP)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XLFcjst13aE"
   },
   "source": [
    "### Indexing and extracting data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YmO8SzC13aF"
   },
   "source": [
    "A `DataFrame` can be indexed in many ways. In this regard, consider various ways of indexing and extracting data from the DataFrame with simple question examples.\n",
    "\n",
    "To extract a separate column, you can use the code `dataframe ['name']`. We use this to answer the question: **What is the share of clients in our DataFrame, who are ready with their default payment in the next month?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8G8Ce_I13aF"
   },
   "outputs": [],
   "source": [
    "print(\"Part of clients =\", '{:.2%}'.format(df[\"default payment next month\"].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frbJ1GQp13aG"
   },
   "source": [
    "`22,12%` is not a good enough indicator for an organization, a business can be broken with such a percentage of credit repayment (every fifth client). The main task of the security department, in this case, is to avoid this state and prevent it in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cauScC7f13aG"
   },
   "source": [
    "Logical indexation by one column of a `DataFrame` is very convenient. It looks like this: `df [p(df['Name']]`, where`  p ` is a certain logical condition that is checked for each element of the `Name` column. The result of such an indexation is a `DataFrame` consisting only of the rows satisfying the condition `p` by the `Name` column.\n",
    "\n",
    "We use this to answer the question: **What are the mean values ​​of the numerical features among the attracted clients?**\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with your Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXXcd0mH13aH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df[\"default payment next month\"] == 1]##YOUR CODE GOES HERE##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEhM_a9KBMnY"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "df[df[\"default payment next month\"] == 1].mean()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqYEmztJY-Vf"
   },
   "source": [
    "Thus, the average age of a typical client is about 35 years (`age` = 35.726), at the same time, their repayment status has been improved during half a year (based on the values of the variables `PAY_0`-`PAY_6`) and they paid with a delay from several weeks (`PAY_0` = `0.668`) to several days (`PAY_6` = `0.112`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ModvYJAq13aH"
   },
   "source": [
    "Combining previous two types of indexation, we will answer the question: **What was the average repayment status (`ars`) in September 2005 for a typical client who was going to pay in the next month**?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ubf3zpbk13aH"
   },
   "outputs": [],
   "source": [
    "ars = round(df[df[\"default payment next month\"] == 1][\"PAY_2\"].mean(), 2) * 100\n",
    "print(\"Average repayment status, which was in September, 2005 for the typical client =\", ars, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9a34m70ZgRa"
   },
   "source": [
    "So, the average repayment status for September, 2005, for a typical client is 46%, it is almost a half of all the clients, which is not bad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95VAbA6Z13aJ"
   },
   "source": [
    "**What part of the clients (`default payment next month` == `1`) had paid duly (`PAY_0` == `-1`) in September, 2005?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9Mx-i4r13aK"
   },
   "outputs": [],
   "source": [
    "pd = round(((df[\"default payment next month\"] == 1).sum() & df[\"PAY_0\"].value_counts()[-1]) / df.shape[0], 3)*100\n",
    "print(\"Part of clients =\", pd, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiSCnAkXOWlP"
   },
   "source": [
    "So, the part of the cliens who had paid duly in September, 2005, is 13,8%. We cannot make a right conclusion about the current security policy based on this indicator only. Thus, we need to expand a set of similar indicators so that we can reach a right conclusion and give necessary recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omZ1hAP1FVIH"
   },
   "source": [
    "## Dataset Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKngsucvXMvE"
   },
   "source": [
    "We are going to show you a general approach to data visualization with the help of a bar plot (based on a `displot` method, [official documentation](https://seaborn.pydata.org/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsPreliminary_data_analysis_dataset_L126905723-2022-01-01)) where the axis representing the data variable is divided into a set of discrete bins and the count of observations falling within each bin is shown using the height of the corresponding bar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7ws023fSBOV"
   },
   "source": [
    "`displot` method allows you to visualize some dependencies among the features. We will do it for numerical features.\n",
    "\n",
    "Firstly, let's show a distribution of the feature `LIMIT_BAL` (this feature will be a general indicator for all our following experiments) for both values of the variable `SEX` (`1` = male; `2` = female).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKJccXjPSzU0"
   },
   "outputs": [],
   "source": [
    "sns.displot(df, x = \"LIMIT_BAL\", col = \"SEX\", multiple = \"dodge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0njjlsQ9s3n4"
   },
   "source": [
    "As you can see, the amount of the given credit (NT dollar) which includes both an individual consumer's credit and their family (supplementary) credit is approximately higher for women.\n",
    "\n",
    "Secondly, let's show a distribution (density in our case) of the feature `LIMIT_BAL` for all four values of the variable `MARRIAGE` (`1` = married; `2` = single; `3` = others) with the help of the parameter `hue`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-gmXgggQIkl"
   },
   "outputs": [],
   "source": [
    "sns.displot(df, x = \"LIMIT_BAL\", hue = \"MARRIAGE\", kind = \"kde\", multiple = \"stack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3Dm_BCsAtMK"
   },
   "source": [
    "As you can see, most of the clients are married with a high density of distribution. The group of single clients comes next. And the group of clients with another relationship state closes our distribution in the third place.\n",
    "\n",
    "At the same time, we don't see a distribution with a state that is equal to `0`, perhaps, it's an outlier in our dataset.\n",
    "\n",
    "In addition, we can use a “small-multiple” approach to visualize the univariate distribution of all the variables in our dataset along with all of their pairwise relationships by `PairGrid` method directly. It will afford more flexibility with only a bit more typing (for instance, for our four features: `SEX`, `EDUCATION`, `MARRIAGE`, `AGE`). This code might take some time to execute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gohTgkuSUGKO"
   },
   "outputs": [],
   "source": [
    "g = sns.PairGrid(df[['SEX', 'EDUCATION', 'MARRIAGE', 'AGE']])\n",
    "g.map_upper(sns.histplot)\n",
    "g.map_lower(sns.kdeplot, fill=True)\n",
    "g.map_diag(sns.histplot, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEAaz2PIFVII"
   },
   "source": [
    "**For each feature, you can build a separate histogram, for instance, let's build it for the variable `AGE`**. Parameter `bins` (you can change it optionally) defines the number of equal-width bins in the range for our histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJOQB60_FVII"
   },
   "outputs": [],
   "source": [
    "df[\"AGE\"].hist(bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjnmCPbFefXl"
   },
   "source": [
    "The histogram shows that most of our clients are between the ages of (approximately) `21-22` and `44`, which corresponds to the actively working part of the population.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1A59i55FVIJ"
   },
   "source": [
    "**Or you can build it for all together:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5oIPZa0FVIJ"
   },
   "outputs": [],
   "source": [
    "df.hist(color = \"k\",\n",
    "        bins = 30,\n",
    "        figsize = (15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gb57x-rPe5mb"
   },
   "source": [
    "A visual analysis of the histograms presented allows us to make preliminary assumptions about the variability of the source data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hQDR1PGFVIK"
   },
   "source": [
    "[**Box Plot** (\"Box and whisker plot\")](https://seaborn.pydata.org/generated/seaborn.boxplot.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsPreliminary_data_analysis_dataset_L126905723-2022-01-01) is useful too. It allows you to compactly visualize the main characteristics of the feature distribution (the median, lower and upper quartile, minimal and maximum, outliers).\n",
    "\n",
    "For instance, let's build a distribution of the variable `LIMIT_BAL` which depends on two other variables `EDUCATION` (`1` = graduate school; `2` = university; `3` = high school; `4` = others) and `SEX`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kopQyqZaJxBT"
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style = \"whitegrid\")\n",
    "sns.boxplot(x = \"EDUCATION\",\n",
    "                 y = \"LIMIT_BAL\",\n",
    "                 hue = \"SEX\",\n",
    "                 data = df,\n",
    "                 linewidth = 2.0,\n",
    "                 dodge = True,\n",
    "                 palette = \"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ci3YxUBgDjB"
   },
   "source": [
    "This plotting shows that people after a graduate school on average have a bigger amount of given credit (NT dollar) than those who have a university or a high school education. In addition, we can see that all the eduaction groups have outlier zones: over `0.5` for the first education level, and about `0.4` for the second and third levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQ5td0XaFVIL"
   },
   "source": [
    "**You can do this by data grouping on any other feature:**\n",
    "\n",
    "Replace `##YOUR CODE GOES HERE##` with your Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtnHOQ3sFVIM"
   },
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE##(x = \"MARRIAGE\",\n",
    "                 y = \"AGE\",\n",
    "                 hue = \"SEX\",\n",
    "                 data = df,\n",
    "                 linewidth = 2.0,\n",
    "                 dodge = True,\n",
    "                 palette = \"Set3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYxhXkrmrCsn"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x = \"MARRIAGE\",\n",
    "                 y = \"AGE\",\n",
    "                 hue = \"SEX\",\n",
    "                 data = df,\n",
    "                 linewidth = 2.0,\n",
    "                 dodge = True,\n",
    "                 palette = \"Set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNzZczbK_jd1"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "sns.boxplot(x = \"MARRIAGE\",\n",
    "                 y = \"AGE\",\n",
    "                 hue = \"SEX\",\n",
    "                 data = df,\n",
    "                 linewidth = 2.0,\n",
    "                 dodge = True,\n",
    "                 palette = \"Set3\")\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUMjKczqTThq"
   },
   "source": [
    "## Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLCT0HXkTThr"
   },
   "source": [
    "In this section you will solve some tasks with the source credit card clients dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7C0DMPdTThs"
   },
   "source": [
    "### Question 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psvWjxS4TThs"
   },
   "source": [
    "Create a list of only 15 IDs of clients with the largest amount of previous payment (NT dollar) in September 2005 and indicate the average of this payment and the average age for these 15 clients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIFR1-sLoKmJ"
   },
   "outputs": [],
   "source": [
    "df.sort_values(by = \"PAY_AMT1\", ascending = False)[[\"PAY_AMT1\", \"AGE\"]].head(15).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unSAHwIPTTht"
   },
   "outputs": [],
   "source": [
    "# Code block for students' answer. \n",
    "# Replace ##YOUR CODE GOES HERE## with your Python code.\n",
    "\n",
    "df.sort_values(by = ##YOUR CODE GOES HERE##, ascending = False)[##YOUR CODE GOES HERE##].head(15)##YOUR CODE GOES HERE##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWKD4lEOmLEo"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "df.sort_values(by = \"PAY_AMT1\", ascending = False)[[\"PAY_AMT1\", \"AGE\"]].head(15).mean()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fs2Wf3RQTThw"
   },
   "source": [
    "### Question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPr0VwEXTThw"
   },
   "source": [
    "Build an output box plot to analyze the clients' age distribution by their education level (`1` = graduate school; `2` = university; `3` = high school; `4` = others) and their marital status (`1` = married; `2` = single).\n",
    "\n",
    "Which group has more observations:\n",
    "\n",
    "1.  single with a high school education level or single with a graduate school education level?\n",
    "2.  with a minimum number of outliers: married with a graduate school or university or high school education?\n",
    "3.  where the average age is higher for all three education levels: for married or single clients?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YO648QDYTThy"
   },
   "outputs": [],
   "source": [
    "# Code block for students' answer.\n",
    "# Replace ##YOUR CODE GOES HERE## with your Python code.\n",
    "\n",
    "##YOUR CODE GOES HERE##(x = \"EDUCATION\",\n",
    "                 y = \"AGE\",\n",
    "                 hue = ##YOUR CODE GOES HERE##,\n",
    "                 data = df,\n",
    "                 linewidth = 2.0,\n",
    "                 dodge = True,\n",
    "                 palette = \"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvUFCih5phqI"
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "sns.boxplot(x = \"EDUCATION\",\n",
    "                 y = \"AGE\",\n",
    "                 hue = \"MARRIAGE\",\n",
    "                 data = df,\n",
    "                 linewidth = 2.0,\n",
    "                 dodge = True,\n",
    "                 palette = \"Set2\")\n",
    "# Answers:\n",
    "# 1. single with graduate school education level\n",
    "# 2. married with high school education\n",
    "# 3. married\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEbNXiqmTTh0"
   },
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_1ZOvXKTTh0"
   },
   "source": [
    "[Sergii Kavun](https://www.linkedin.com/in/sergii-kavun/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsPreliminary_data_analysis_dataset_L126905723-2022-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHpM3GyTTTh2"
   },
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrgMUa_ETTh2"
   },
   "source": [
    "| Date (YYYY-MM-DD) | Version | Changed By    | Change Description   |\n",
    "| ----------------- | ------- | ------------- | -------------------- |\n",
    "| 2021-05-25        | 0.21    | Kavun, Sergii | Code refactoring     |\n",
    "| 2021-05-20        | 0.2     | Kavun, Sergii | Translate to english |\n",
    "| 2021-05-19        | 0.1     | Kavun, Sergii | Created Lab          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma5EMufCTTh2"
   },
   "source": [
    "Copyright © 2021 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
